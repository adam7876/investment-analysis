#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆæ•¸æ“šçˆ¬èŸ² - æ•´åˆå¤šå€‹æ•¸æ“šæºæé«˜å¯ä¿¡åº¦
åŒ…å«ï¼šFX678ã€CME FedWatchã€Investing.comã€CNN Fear & Greedç­‰
"""

import requests
from bs4 import BeautifulSoup
import re
import json
import time
import random
from datetime import datetime
from loguru import logger
from typing import Dict, List, Any, Optional
from config import Config

class EnhancedDataScraper:
    """å¢å¼·ç‰ˆæ•¸æ“šçˆ¬èŸ²"""
    
    def __init__(self):
        self.session = requests.Session()
        self._setup_session()
        
        # æ–°å¢æ•¸æ“šæºURL
        self.enhanced_urls = {
            'fx678_cpi': 'https://rl.fx678.com/content/id/112015032410000087.html',
            'cme_fedwatch': 'https://www.cmegroup.com/markets/interest-rates/cme-fedwatch-tool.html',
            'investing_nonfarm': 'https://hk.investing.com/economic-calendar/nonfarm-payrolls-227',
            'cnn_fear_greed': 'https://edition.cnn.com/markets/fear-and-greed'
        }
    
    def _setup_session(self):
        """è¨­ç½®è«‹æ±‚æœƒè©±"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        }
        self.session.headers.update(headers)
    
    def get_page_safely(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """å®‰å…¨ç²å–ç¶²é å…§å®¹"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                
                # éš¨æ©Ÿå»¶é²é¿å…è¢«å°
                time.sleep(random.uniform(1, 3))
                
                return BeautifulSoup(response.content, 'html.parser')
                
            except Exception as e:
                logger.warning(f"ç²å–é é¢å¤±æ•— (å˜—è©¦ {attempt + 1}/{retries}): {url}, éŒ¯èª¤: {str(e)}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
                else:
                    logger.error(f"æœ€çµ‚ç²å–å¤±æ•—: {url}")
                    return None
    
    def scrape_fx678_cpi(self) -> Optional[Dict[str, Any]]:
        """çˆ¬å–FX678 CPIæ•¸æ“š"""
        logger.info("ğŸ›ï¸ é–‹å§‹çˆ¬å– FX678 CPI æ•¸æ“š...")
        
        try:
            soup = self.get_page_safely(self.enhanced_urls['fx678_cpi'])
            if not soup:
                return None
            
            # æå–CPIæ•¸æ“š
            cpi_data = self._extract_fx678_cpi_data(soup)
            
            if cpi_data:
                logger.info(f"âœ… FX678 CPI æ•¸æ“šç²å–æˆåŠŸ: {cpi_data}")
                return {
                    'source': 'FX678',
                    'timestamp': datetime.now().isoformat(),
                    'data': cpi_data,
                    'reliability': 'high'
                }
            else:
                logger.warning("âš ï¸ FX678 CPI æ•¸æ“šæå–å¤±æ•—")
                return None
                
        except Exception as e:
            logger.error(f"âŒ FX678 CPI çˆ¬å–å¤±æ•—: {str(e)}")
            return None
    
    def scrape_cme_fedwatch(self) -> Optional[Dict[str, Any]]:
        """çˆ¬å–CME FedWatchåˆ©ç‡é æœŸæ•¸æ“š"""
        logger.info("ğŸ“ˆ é–‹å§‹çˆ¬å– CME FedWatch æ•¸æ“š...")
        
        try:
            soup = self.get_page_safely(self.enhanced_urls['cme_fedwatch'])
            if not soup:
                return None
            
            # æå–åˆ©ç‡é æœŸæ•¸æ“š
            fed_data = self._extract_cme_fedwatch_data(soup)
            
            if fed_data:
                logger.info(f"âœ… CME FedWatch æ•¸æ“šç²å–æˆåŠŸ: {fed_data}")
                return {
                    'source': 'CME FedWatch',
                    'timestamp': datetime.now().isoformat(),
                    'data': fed_data,
                    'reliability': 'very_high'
                }
            else:
                logger.warning("âš ï¸ CME FedWatch æ•¸æ“šæå–å¤±æ•—")
                return None
                
        except Exception as e:
            logger.error(f"âŒ CME FedWatch çˆ¬å–å¤±æ•—: {str(e)}")
            return None
    
    def scrape_investing_employment(self) -> Optional[Dict[str, Any]]:
        """çˆ¬å–Investing.comå°±æ¥­æ•¸æ“š"""
        logger.info("ğŸ‘¥ é–‹å§‹çˆ¬å– Investing.com å°±æ¥­æ•¸æ“š...")
        
        try:
            soup = self.get_page_safely(self.enhanced_urls['investing_nonfarm'])
            if not soup:
                return None
            
            # æå–å°±æ¥­æ•¸æ“š
            employment_data = self._extract_investing_employment_data(soup)
            
            if employment_data:
                logger.info(f"âœ… Investing.com å°±æ¥­æ•¸æ“šç²å–æˆåŠŸ: {employment_data}")
                return {
                    'source': 'Investing.com',
                    'timestamp': datetime.now().isoformat(),
                    'data': employment_data,
                    'reliability': 'high'
                }
            else:
                logger.warning("âš ï¸ Investing.com å°±æ¥­æ•¸æ“šæå–å¤±æ•—")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Investing.com å°±æ¥­æ•¸æ“šçˆ¬å–å¤±æ•—: {str(e)}")
            return None
    
    def scrape_cnn_fear_greed(self) -> Optional[Dict[str, Any]]:
        """çˆ¬å–CNN Fear & Greed Index"""
        logger.info("ğŸ˜¨ é–‹å§‹çˆ¬å– CNN Fear & Greed Index...")
        
        try:
            soup = self.get_page_safely(self.enhanced_urls['cnn_fear_greed'])
            if not soup:
                return None
            
            # æå–ææ‡¼è²ªå©ªæŒ‡æ•¸
            fear_greed_data = self._extract_cnn_fear_greed_data(soup)
            
            if fear_greed_data:
                logger.info(f"âœ… CNN Fear & Greed æ•¸æ“šç²å–æˆåŠŸ: {fear_greed_data}")
                return {
                    'source': 'CNN Fear & Greed',
                    'timestamp': datetime.now().isoformat(),
                    'data': fear_greed_data,
                    'reliability': 'high'
                }
            else:
                logger.warning("âš ï¸ CNN Fear & Greed æ•¸æ“šæå–å¤±æ•—")
                return None
                
        except Exception as e:
            logger.error(f"âŒ CNN Fear & Greed çˆ¬å–å¤±æ•—: {str(e)}")
            return None
    
    def scrape_all_enhanced_sources(self) -> Dict[str, Any]:
        """çˆ¬å–æ‰€æœ‰å¢å¼·æ•¸æ“šæº"""
        logger.info("ğŸš€ é–‹å§‹çˆ¬å–æ‰€æœ‰å¢å¼·æ•¸æ“šæº...")
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'sources': {},
            'summary': {},
            'reliability_score': 0
        }
        
        # çˆ¬å–å„å€‹æ•¸æ“šæº
        scrapers = [
            ('fx678_cpi', self.scrape_fx678_cpi),
            ('cme_fedwatch', self.scrape_cme_fedwatch),
            ('investing_employment', self.scrape_investing_employment),
            ('cnn_fear_greed', self.scrape_cnn_fear_greed)
        ]
        
        successful_sources = 0
        total_sources = len(scrapers)
        
        for source_name, scraper_func in scrapers:
            try:
                data = scraper_func()
                if data:
                    results['sources'][source_name] = data
                    successful_sources += 1
                    
                    # æ ¹æ“šå¯é æ€§åŠ åˆ†
                    reliability = data.get('reliability', 'medium')
                    if reliability == 'very_high':
                        results['reliability_score'] += 30
                    elif reliability == 'high':
                        results['reliability_score'] += 25
                    else:
                        results['reliability_score'] += 15
                
                # éš¨æ©Ÿå»¶é²é¿å…éæ–¼é »ç¹è«‹æ±‚
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                logger.error(f"âŒ {source_name} çˆ¬å–å¤±æ•—: {str(e)}")
        
        # ç”Ÿæˆæ‘˜è¦
        results['summary'] = {
            'total_sources': total_sources,
            'successful_sources': successful_sources,
            'success_rate': f"{(successful_sources/total_sources)*100:.1f}%",
            'reliability_score': results['reliability_score'],
            'data_quality': self._assess_data_quality(results['sources'])
        }
        
        logger.info(f"âœ… å¢å¼·æ•¸æ“šæºçˆ¬å–å®Œæˆï¼ŒæˆåŠŸç‡: {results['summary']['success_rate']}")
        return results
    
    def _extract_fx678_cpi_data(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """æå–FX678 CPIæ•¸æ“š"""
        try:
            text = soup.get_text()
            
            # å°‹æ‰¾CPIç›¸é—œæ•¸æ“šçš„å¤šç¨®æ¨¡å¼
            patterns = [
                r'CPI.*?å¹´ç‡.*?([+-]?\d+\.?\d*)%',
                r'CPI.*?([+-]?\d+\.?\d*)%',
                r'é€šè†¨.*?([+-]?\d+\.?\d*)%',
                r'æ¶ˆè²»è€…ç‰©åƒ¹.*?([+-]?\d+\.?\d*)%',
                r'æ ¸å¿ƒCPI.*?([+-]?\d+\.?\d*)%'
            ]
            
            cpi_values = []
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    try:
                        value = float(match)
                        if -5 <= value <= 15:  # åˆç†çš„CPIç¯„åœ
                            cpi_values.append(value)
                    except ValueError:
                        continue
            
            if cpi_values:
                # å–æœ€å¸¸è¦‹çš„å€¼æˆ–å¹³å‡å€¼
                avg_cpi = sum(cpi_values) / len(cpi_values)
                return {
                    'cpi_annual': round(avg_cpi, 1),
                    'values_found': cpi_values,
                    'status': self._classify_inflation_status(avg_cpi),
                    'indicator': 'CPI Annual Rate'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"FX678 CPIæ•¸æ“šæå–å¤±æ•—: {str(e)}")
            return None
    
    def _extract_cme_fedwatch_data(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """æå–CME FedWatchæ•¸æ“š"""
        try:
            text = soup.get_text()
            
            # å°‹æ‰¾åˆ©ç‡é æœŸç›¸é—œæ•¸æ“š
            patterns = [
                r'(\d+\.?\d*)%.*?probability',
                r'probability.*?(\d+\.?\d*)%',
                r'(\d+\.?\d*)\s*basis\s*points',
                r'(\d+\.?\d*)%.*?chance',
                r'(\d+\.?\d*)\s*bps'
            ]
            
            probabilities = []
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    try:
                        value = float(match)
                        if 0 <= value <= 100:  # æ¦‚ç‡ç¯„åœ
                            probabilities.append(value)
                    except ValueError:
                        continue
            
            if probabilities:
                # æ‰¾å‡ºæœ€é«˜æ¦‚ç‡
                max_prob = max(probabilities)
                return {
                    'max_probability': max_prob,
                    'all_probabilities': probabilities,
                    'fed_outlook': self._interpret_fed_probability(max_prob),
                    'indicator': 'Fed Rate Probability'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"CME FedWatchæ•¸æ“šæå–å¤±æ•—: {str(e)}")
            return None
    
    def _extract_investing_employment_data(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """æå–Investing.comå°±æ¥­æ•¸æ“š"""
        try:
            text = soup.get_text()
            
            # å°‹æ‰¾å°±æ¥­ç›¸é—œæ•¸æ“š
            patterns = [
                r'éè¾².*?(\d+\.?\d*)[KM]?',
                r'nonfarm.*?(\d+\.?\d*)[KM]?',
                r'unemployment.*?(\d+\.?\d*)%',
                r'å¤±æ¥­ç‡.*?(\d+\.?\d*)%',
                r'(\d+\.?\d*)[KM]?\s*jobs'
            ]
            
            employment_data = {}
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    try:
                        value = float(match)
                        if 'unemployment' in pattern.lower() or 'å¤±æ¥­' in pattern:
                            if 0 <= value <= 20:  # å¤±æ¥­ç‡åˆç†ç¯„åœ
                                employment_data['unemployment_rate'] = value
                        else:
                            if 0 <= value <= 1000:  # å°±æ¥­äººæ•¸åˆç†ç¯„åœ
                                employment_data['nonfarm_payrolls'] = value
                    except ValueError:
                        continue
            
            if employment_data:
                return {
                    **employment_data,
                    'employment_health': self._assess_employment_health(employment_data),
                    'indicator': 'Employment Data'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Investing.comå°±æ¥­æ•¸æ“šæå–å¤±æ•—: {str(e)}")
            return None
    
    def _extract_cnn_fear_greed_data(self, soup: BeautifulSoup) -> Optional[Dict[str, Any]]:
        """æå–CNN Fear & Greedæ•¸æ“š"""
        try:
            text = soup.get_text()
            
            # å°‹æ‰¾ææ‡¼è²ªå©ªæŒ‡æ•¸
            patterns = [
                r'fear.*?greed.*?(\d+)',
                r'(\d+).*?fear.*?greed',
                r'index.*?(\d+)',
                r'greed.*?(\d+)',
                r'fear.*?(\d+)'
            ]
            
            index_values = []
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    try:
                        value = int(match)
                        if 0 <= value <= 100:  # æŒ‡æ•¸ç¯„åœ
                            index_values.append(value)
                    except ValueError:
                        continue
            
            if index_values:
                # å–æœ€å¯èƒ½çš„å€¼ï¼ˆå‡ºç¾é »ç‡æœ€é«˜æˆ–ä¸­ä½æ•¸ï¼‰
                avg_index = sum(index_values) / len(index_values)
                return {
                    'fear_greed_index': round(avg_index),
                    'values_found': index_values,
                    'sentiment': self._classify_market_sentiment(avg_index),
                    'interpretation': self._interpret_sentiment(avg_index),
                    'indicator': 'Fear & Greed Index'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"CNN Fear & Greedæ•¸æ“šæå–å¤±æ•—: {str(e)}")
            return None
    
    def _classify_inflation_status(self, cpi_value: float) -> str:
        """åˆ†é¡é€šè†¨ç‹€æ³"""
        if cpi_value >= 5:
            return "é«˜é€šè†¨"
        elif cpi_value >= 3:
            return "ä¸­åº¦é€šè†¨"
        elif cpi_value >= 2:
            return "ç›®æ¨™é€šè†¨"
        elif cpi_value >= 0:
            return "ä½é€šè†¨"
        else:
            return "é€šç¸®"
    
    def _interpret_fed_probability(self, probability: float) -> str:
        """è§£è®€è¯æº–æœƒåˆ©ç‡é æœŸ"""
        if probability >= 80:
            return "é«˜åº¦ç¢ºå®š"
        elif probability >= 60:
            return "å¾ˆå¯èƒ½"
        elif probability >= 40:
            return "å¯èƒ½"
        elif probability >= 20:
            return "ä¸å¤ªå¯èƒ½"
        else:
            return "æ¥µä¸å¯èƒ½"
    
    def _assess_employment_health(self, data: Dict) -> str:
        """è©•ä¼°å°±æ¥­å¸‚å ´å¥åº·åº¦"""
        unemployment = data.get('unemployment_rate', 5)
        nonfarm = data.get('nonfarm_payrolls', 200)
        
        if unemployment < 4 and nonfarm > 250:
            return "å¼·å‹"
        elif unemployment < 5 and nonfarm > 150:
            return "å¥åº·"
        elif unemployment < 6:
            return "æº«å’Œ"
        else:
            return "ç–²å¼±"
    
    def _classify_market_sentiment(self, index_value: float) -> str:
        """åˆ†é¡å¸‚å ´æƒ…ç·’"""
        if index_value >= 75:
            return "æ¥µåº¦è²ªå©ª"
        elif index_value >= 55:
            return "è²ªå©ª"
        elif index_value >= 45:
            return "ä¸­æ€§"
        elif index_value >= 25:
            return "ææ‡¼"
        else:
            return "æ¥µåº¦ææ‡¼"
    
    def _interpret_sentiment(self, score: float) -> str:
        """è§£è®€å¸‚å ´æƒ…ç·’"""
        if score > 75:
            return "å¸‚å ´æ¥µåº¦è²ªå©ªï¼Œå»ºè­°è¬¹æ…ï¼Œå¯èƒ½æ¥è¿‘é ‚éƒ¨"
        elif score > 60:
            return "å¸‚å ´åå‘è²ªå©ªï¼Œé©åº¦åƒèˆ‡ä½†éœ€è¨­å®šåœæ"
        elif score > 40:
            return "å¸‚å ´æƒ…ç·’ä¸­æ€§ï¼Œå¹³è¡¡æ“ä½œç­‰å¾…æ˜ç¢ºä¿¡è™Ÿ"
        elif score > 25:
            return "å¸‚å ´åå‘ææ‡¼ï¼Œå¯å°‹æ‰¾å„ªè³ªæ¨™çš„é€¢ä½å¸ƒå±€"
        else:
            return "å¸‚å ´æ¥µåº¦ææ‡¼ï¼Œå¾€å¾€æ˜¯è²·å…¥æ©Ÿæœƒä½†éœ€åˆ†æ‰¹é€²å ´"
    
    def _assess_data_quality(self, sources: Dict) -> str:
        """è©•ä¼°æ•¸æ“šå“è³ª"""
        if len(sources) >= 3:
            return "å„ªç§€"
        elif len(sources) >= 2:
            return "è‰¯å¥½"
        elif len(sources) >= 1:
            return "ä¸€èˆ¬"
        else:
            return "ä¸è¶³" 